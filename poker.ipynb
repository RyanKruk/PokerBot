{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: treys in /home/krukr/.conda/envs/apollo/lib/python3.11/site-packages (0.1.8)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Collecting pokerkit\n",
      "  Downloading pokerkit-0.5.4-py3-none-any.whl.metadata (17 kB)\n",
      "Downloading pokerkit-0.5.4-py3-none-any.whl (99 kB)\n",
      "Installing collected packages: pokerkit\n",
      "Successfully installed pokerkit-0.5.4\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install treys\n",
    "%pip install pokerkit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pokerkit import Automation, NoLimitTexasHoldem, State, calculate_hand_strength, parse_range, Card, Deck, StandardHighHand\n",
    "from typing import List\n",
    "from tqdm import tqdm\n",
    "from enum import Enum\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Agent Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Action(Enum):\n",
    "    FOLD = 0\n",
    "    CALL = 1\n",
    "    BET = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init__(self, name: str = None, starting_stack: int = 0):\n",
    "        self.name = name\n",
    "        self.starting_stack = starting_stack\n",
    "        self.stack = starting_stack\n",
    "\n",
    "    def reset(self):\n",
    "        self.stack = self.starting_stack\n",
    "        return self\n",
    "    \n",
    "    def pi_action_generator(self, state: State) -> dict:\n",
    "        pass\n",
    "\n",
    "    def get_player_state(self, state: State, player_index: int) -> List:\n",
    "        # Calculate percent chance of winning\n",
    "        win_percent = self._calculate_strength(state, player_index)\n",
    "        \n",
    "        stack = state.stacks[player_index]\n",
    "        pot = state.total_pot_amount\n",
    "\n",
    "        min_bet = state.min_completion_betting_or_raising_to_amount\n",
    "        max_bet = state.max_completion_betting_or_raising_to_amount\n",
    "        \n",
    "        if min_bet is None:\n",
    "            min_bet = 4\n",
    "        if max_bet is None:\n",
    "            max_bet = stack\n",
    "        \n",
    "        return [win_percent, stack, pot, min_bet, max_bet]\n",
    "\n",
    "    def _get_valid_actions(self, state: State):\n",
    "        if state.can_fold():\n",
    "            yield Action.FOLD\n",
    "        if state.can_check_or_call():\n",
    "            yield Action.CALL\n",
    "        if state.can_complete_bet_or_raise_to():\n",
    "            yield Action.BET\n",
    "\n",
    "    def _calculate_strength(self, state: State, player_index: int, samples: int = 500) -> float:\n",
    "        return calculate_hand_strength(\n",
    "            state.player_count,\n",
    "            parse_range(''.join([str(c.rank + c.suit) for c in state.hole_cards[player_index]])),\n",
    "            Card.parse(''.join([str(c[0].rank + c[0].suit) for c in state.board_cards])),\n",
    "            2,\n",
    "            5,\n",
    "            Deck.STANDARD,\n",
    "            (StandardHighHand,),\n",
    "            sample_count=samples\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExampleRandomAgent(Agent):\n",
    "    def __init__(self, name: str = None, stack: int = 1000):\n",
    "        super().__init__(name, stack)\n",
    "\n",
    "    def pi_action_generator(self, state: State) -> dict:\n",
    "        valid_actions = list(self._get_valid_actions(state))\n",
    "            \n",
    "        valid_bet_low = state.min_completion_betting_or_raising_to_amount\n",
    "        valid_bet_high = state.max_completion_betting_or_raising_to_amount\n",
    "        chosen_action = np.random.choice(valid_actions)\n",
    "\n",
    "        bet_size = 0\n",
    "        if chosen_action is Action.BET:\n",
    "            bet_size = round(np.random.uniform(valid_bet_low, valid_bet_high))\n",
    "\n",
    "        table_action = {\n",
    "            'table_action': chosen_action,\n",
    "            'bet_size': bet_size\n",
    "        }\n",
    "        return table_action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n",
    "class PolicyNetwork(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(PolicyNetwork, self).__init__()\n",
    "        self.hidden_layer = nn.Linear(input_size, hidden_size)    #the hidden layer with hidden_size neurons\n",
    "        #nn.init.xavier_uniform_(self.hidden_layer.weight)     # Initialize the weights with Xavier initialization\n",
    "        nn.init.normal_(self.hidden_layer.weight, mean = 0, std = 0.01)\n",
    "        nn.init.normal_(self.hidden_layer.bias, mean = 0, std = 0.01)\n",
    "        self.action_output = nn.Linear(hidden_size, 3)    #the output layer with outputs as prob of stopping, mean, and variance of normal\n",
    "        self.bet_output = nn.Linear(hidden_size, 1)    #the output layer with outputs as prob of stopping, mean, and variance of normal\n",
    "\n",
    "        \n",
    "    #@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n",
    "    def forward(self, s):\n",
    "        '''A function to do the forward pass\n",
    "            Takes:\n",
    "                s -- the state representation\n",
    "            Returns:\n",
    "                a tensor of probabilities\n",
    "        '''\n",
    "        s = torch.relu(self.hidden_layer(s))    #pass through the hidden layer\n",
    "        a = self.action_output(s)\n",
    "        a = torch.softmax(a, dim=1)\n",
    "        \n",
    "        b = self.bet_output(s)\n",
    "        b = torch.exp(b)/(1 + torch.exp(b))\n",
    "        return a, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n",
    "class PolicyAgent(Agent):\n",
    "    #@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@    \n",
    "    def __init__(self, name, stack, config):\n",
    "        super().__init__(name, stack)    #init the parent class\n",
    "        self.config = config\n",
    "        self.mu_s2 = PolicyNetwork(5, self.config['hidden_layer_size'])    #init the policy model\n",
    "        self.optimizer = optim.Adam(self.mu_s2.parameters(), lr=self.config['learning_rate'])    #init the optimizer\n",
    "\n",
    "    #@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@   \n",
    "    def action_probs(self, a, s):\n",
    "        '''A function to compute the logged action probabilities.  This will used used for gradient updates.\n",
    "            Takes:\n",
    "                a -- -1 (stop) or float in [0,1]\n",
    "                state -- float in [0,100]\n",
    "            Returns:\n",
    "                torch tensor\n",
    "        '''\n",
    "        actions, bet_ratio = self.mu_s2(torch.tensor([s]))    #compute stop prob, mean, sd\n",
    "\n",
    "        if a[0] == Action.FOLD:   #if the action was to stop...\n",
    "            log_p = torch.log(actions[0][0])\n",
    "        elif a[0] == Action.CALL:    #if the action was to ask for a...\n",
    "            log_p = torch.log(actions[0][1])\n",
    "        else:    #if the action was to ask for a in [0,1]...\n",
    "            action_log_p = torch.log(actions[0][2])\n",
    "\n",
    "            low = torch.max(bet_ratio[0][0] - self.config['action_var'], torch.tensor([0.0]))\n",
    "            high = torch.min(bet_ratio[0][0] + self.config['action_var'], torch.tensor([1.0]))\n",
    "            U = torch.distributions.Uniform(low, high)\n",
    "            bet_log_p = U.log_prob(torch.tensor(a[1])) \n",
    "\n",
    "            log_p = (action_log_p + bet_log_p)[0]\n",
    "        return log_p\n",
    "\n",
    "    #@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@   \n",
    "    def pi_action_generator(self, s: State):\n",
    "        '''A function to generate an action.  This will be used to generate the data.\n",
    "            Takes:\n",
    "                state -- float in [0,100]\n",
    "            Returns:\n",
    "                 -1 or a in [0,1]\n",
    "\n",
    "        '''\n",
    "        state = self.get_player_state(s, s.actor_index)    #get the state\n",
    "        \n",
    "        actions, bet_ratio = self.mu_s2(torch.tensor([state]))    #generate policy parameters\n",
    "        actions, bet_ratio = torch.squeeze(actions), torch.squeeze(bet_ratio)    #reshape\n",
    "\n",
    "        fold, call, bet = self._calculate_valid_action_values(s, actions)\n",
    "        bet_size = 0    #init the bet size\n",
    "        action_chance = np.random.uniform()    #generate a random number to decide what to do\n",
    "        if action_chance < float(fold):\n",
    "            a = Action.FOLD    #set the relevant action\n",
    "        elif action_chance < float(fold) + float(call):\n",
    "            a = Action.CALL\n",
    "        else:    #if not...\n",
    "            a = Action.BET    #set the relevant action\n",
    "            min_bet_size, max_bet_size = state[-2], state[-1]    #pull out the min and max bet sizes\n",
    "\n",
    "            low = torch.max(bet_ratio - self.config['action_var'],torch.tensor([0.0]))\n",
    "            high = torch.min(bet_ratio + self.config['action_var'],torch.tensor([1.0]))\n",
    "            U = torch.distributions.Uniform(low, high)\n",
    "            bet_ratio = float(U.sample())\n",
    "\n",
    "            bet_size = int((bet_ratio * (max_bet_size - min_bet_size)) + min_bet_size)    #compute the bet size\n",
    "            \n",
    "        return {\n",
    "            'table_action': a,\n",
    "            'bet_size': bet_size,\n",
    "            'bet_ratio': bet_ratio\n",
    "        }\n",
    "\n",
    "    #@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n",
    "    def objective(self, log_probs, episode_return, b):\n",
    "        '''A function to compute the objective\n",
    "            Takes:\n",
    "                log_probs -- tensor, the output from the forward pass\n",
    "                causal_return -- tensor, the causal return as defined in lecture\n",
    "                b -- float, the baseline as defined in lecture\n",
    "        '''\n",
    "        return -torch.sum(log_probs * (episode_return - b))\n",
    "\n",
    "    #@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n",
    "    def update_pi(self, batch):\n",
    "        '''A function to update the gradient of the agent.\n",
    "            Takes:\n",
    "                batch -- a list of dictionary containing episode histories\n",
    "        '''\n",
    "        objective = []    #init the objectives per episode   \n",
    "        for j in range(self.config['N']):    #loop over episodes\n",
    "            batch_j = batch[j]    #pull out episode j\n",
    "            log_probs = []    #init the log probs for this episode\n",
    "            for s, a in zip(batch_j['states'][:len(batch_j['states'])-1], batch_j['actions']):    #loop over state action pairs\n",
    "                log_prob = self.action_probs(a, s)    #compute the log prob for this state action pair\n",
    "                log_probs.append(log_prob)    #record\n",
    "            log_probs = torch.stack(log_probs)    #reshape to compute gradient over the whole episode\n",
    "            if self.config['causal_return']:    #if we use causal returns...\n",
    "                batch_j_reward = batch_j['causal_return']    #set that\n",
    "            else:    #if not...\n",
    "                batch_j_reward = batch_j['total_return']    #use the total discounted reward\n",
    "\n",
    "            objective.append(self.objective(log_probs, batch_j_reward, batch_j['baseline']))    #compute the objective function and record\n",
    "        \n",
    "        objective = torch.mean(torch.stack(objective))    #reshape\n",
    "        \n",
    "        #run the backward pass to compute gradients\n",
    "        self.optimizer.zero_grad()    #zero gradients from the previous step\n",
    "        objective.backward()    #compute gradients\n",
    "        self.optimizer.step()    #update policy network parameters\\n\"\n",
    "        \n",
    "    def _calculate_valid_action_values(self, state: State, actions: torch.Tensor):\n",
    "        valid_actions = list(self._get_valid_actions(state))\n",
    "        actions = actions.detach().numpy()\n",
    "        \n",
    "        valid_action_values = actions[[Action.FOLD in valid_actions, Action.CALL in valid_actions, Action.BET in valid_actions]]\n",
    "        valid_action_values = valid_action_values / max(valid_action_values)\n",
    "        valid_action_values = np.exp(valid_action_values)/np.sum(np.exp(valid_action_values))\n",
    "\n",
    "        fold, call, bet = 0, 0, 0\n",
    "\n",
    "        num_actions = 0\n",
    "        if Action.FOLD in valid_actions:\n",
    "            fold = valid_action_values[num_actions]\n",
    "            num_actions += 1\n",
    "        if Action.CALL in valid_actions:\n",
    "            call = valid_action_values[num_actions]\n",
    "            num_actions += 1\n",
    "        if Action.BET in valid_actions:\n",
    "            bet = valid_action_values[num_actions]\n",
    "            num_actions += 1\n",
    "\n",
    "        return fold, call, bet\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TexasHoldemEnvironment():\n",
    "    def __init__(self, config):\n",
    "        self.config = config\n",
    "        self.reset_game()\n",
    "    \n",
    "    def reset_game(self):\n",
    "        self.game = NoLimitTexasHoldem(\n",
    "            automations= (\n",
    "                Automation.ANTE_POSTING,\n",
    "                Automation.BET_COLLECTION,\n",
    "                Automation.BLIND_OR_STRADDLE_POSTING,\n",
    "                Automation.CARD_BURNING,\n",
    "                Automation.HOLE_DEALING,\n",
    "                Automation.BOARD_DEALING,\n",
    "                Automation.HOLE_CARDS_SHOWING_OR_MUCKING,\n",
    "                Automation.HAND_KILLING,\n",
    "                Automation.CHIPS_PUSHING,\n",
    "                Automation.CHIPS_PULLING\n",
    "            ),\n",
    "            ante_trimming_status=True,  # Uniform antes?\n",
    "            raw_antes=0,  # Antes\n",
    "            raw_blinds_or_straddles=self.config['blinds'],  # Blinds\n",
    "            min_bet=self.config['min_bet'],  # Minimum bet\n",
    "        )\n",
    "        self.stacks = np.array([self.config['starting_stack'] for _ in range(self.config['player_count'])])  # Observed Agent stack will always be at position 0\n",
    "        self.poker_round = None\n",
    "        \n",
    "    def reset_round(self, agents: List[Agent]):\n",
    "        agent_stacks = [agent.stack for agent in agents]\n",
    "        self.poker_round = self.game(raw_starting_stacks=agent_stacks, player_count=agent_stacks.__len__())\n",
    "\n",
    "    def step(self, action):\n",
    "        player_action = action['table_action']\n",
    "        bet_amount = action['bet_size']\n",
    "\n",
    "        # Update the environment state with the player action\n",
    "        if player_action == Action.FOLD:\n",
    "            self.poker_round.fold()\n",
    "        elif player_action == Action.CALL:\n",
    "            self.poker_round.check_or_call()\n",
    "        elif player_action == Action.BET:\n",
    "            self.poker_round.complete_bet_or_raise_to(bet_amount)\n",
    "        \n",
    "        done = not self.poker_round.status  # Check if the round is done\n",
    "\n",
    "        # Update the environment stacks if the round is done\n",
    "        if done:\n",
    "            self.stacks = self.poker_round.stacks\n",
    "\n",
    "        return {'state': self.poker_round, 'reward': self.config['ongoing_reward'], 'done': done}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Running the Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_config = {\n",
    "    'gamma': 0.1,\n",
    "    'action_var': 1,\n",
    "    'hidden_layer_size': 16,\n",
    "    'learning_rate': 0.001,\n",
    "    'B': 10,\n",
    "    'N': 4,\n",
    "    'causal_return': True,\n",
    "    'baseline': True\n",
    "}   # Set the configuration for the agent\n",
    "env_config = {\n",
    "    'player_count': 4,\n",
    "    'blinds': (2, 4),\n",
    "    'min_bet': 4,\n",
    "    'starting_stack': 200,\n",
    "    'ongoing_reward': 0.5\n",
    "}   # Set the configuration for the environment\n",
    "\n",
    "# Define the players in the game with Agent objects\n",
    "# The first player in the list will be the tracked player\n",
    "tracked_agent = PolicyAgent(name='Policy Agent', stack=env_config['starting_stack'], config=agent_config)  # Tracked player\n",
    "players: List[Agent] = [\n",
    "    tracked_agent,\n",
    "    ExampleRandomAgent(name='Random Agent 1', stack=env_config['starting_stack']),\n",
    "    ExampleRandomAgent(name='Random Agent 2', stack=env_config['starting_stack']),\n",
    "    ExampleRandomAgent(name='Random Agent 3', stack=env_config['starting_stack'])\n",
    "]\n",
    "# Randomly select a player to be the dealer\n",
    "player_offset = np.random.randint(0, len(players))\n",
    "# Create the environment with the configuration\n",
    "env = TexasHoldemEnvironment(env_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Poker Batches of 4 Games: 100%|██████████| 10/10 [05:00<00:00, 30.02s/it]\n"
     ]
    }
   ],
   "source": [
    "#run training loop\n",
    "tracked_agent_stack_sizes = []\n",
    "\n",
    "for b in tqdm(range(agent_config['B']), desc=f'Poker Batches of {agent_config[\"N\"]} Games'):    #loop over batches\n",
    "    baseline = 0    #init the baseline\n",
    "    batch = []    #init the batch\n",
    "    for _ in range(agent_config['N']):    #loop over episodes\n",
    "        active_players = [agent.reset() for agent in players]\n",
    "        game_stack_sizes = [env_config['starting_stack']]\n",
    "        env.reset_game()\n",
    "        env.reset_round(active_players)\n",
    "        \n",
    "        gamma_array = [1]    #init the discounting\n",
    "        states = []    #init the state history\n",
    "        actions = []    #init the action history\n",
    "        rewards = []    #init the reward history\n",
    "        \n",
    "        # Play rounds until there is only one player left or the tracked player is eliminated\n",
    "        while len(active_players) > 1 and tracked_agent in active_players:\n",
    "            round_done = False   #Set the stopping condition\n",
    "            \n",
    "            # Calculate amount to offset players to get a new dealer\n",
    "            player_offset = (player_offset + 1) % len(active_players)\n",
    "            # Rotate players based on offset\n",
    "            active_players = active_players[player_offset:] + active_players[:player_offset]\n",
    "            \n",
    "            # Reset the environment for a round of poker\n",
    "            env.reset_round(active_players)\n",
    "            \n",
    "            # Record the state of the tracked player\n",
    "            states.append(tracked_agent.get_player_state(env.poker_round, ((env.poker_round.actor_index - player_offset) + len(active_players)) % len(active_players)))\n",
    "            \n",
    "            # Play the round until the round is over\n",
    "            while not round_done:\n",
    "                # Get the current acting player and their index\n",
    "                current_player_index = env.poker_round.actor_index\n",
    "                current_player: Agent = active_players[current_player_index]\n",
    "                \n",
    "                # Get the action from the current acting player\n",
    "                action = current_player.pi_action_generator(env.poker_round)\n",
    "                # Step the environment with the player's action\n",
    "                update = env.step(action)\n",
    "                \n",
    "                # Update the stopping condition\n",
    "                round_done = update['done']\n",
    "\n",
    "                # If the current player is the tracked player, record the data\n",
    "                if current_player == tracked_agent:\n",
    "                    states.append(tracked_agent.get_player_state(update['state'], current_player_index) if action['table_action'] != Action.FOLD and not round_done else np.zeros(5))\n",
    "                    actions.append([action['table_action'], action['bet_ratio']])\n",
    "                    rewards.append(update['reward'] if action['table_action'] != Action.FOLD else 0)\n",
    "                    gamma_array.append(gamma_array[-1] * agent_config['gamma'])\n",
    "                    \n",
    "            \n",
    "            # Update stacks for each player\n",
    "            for agent, stack in zip(active_players, env.stacks):\n",
    "                agent.stack = stack\n",
    "                \n",
    "            #Set the reward for the last state to be the terminal reward\n",
    "            if len(rewards) > 0:\n",
    "                rewards[-1] = tracked_agent.stack   \n",
    "            else:\n",
    "                rewards.append(tracked_agent.stack)\n",
    "                \n",
    "            game_stack_sizes.append(rewards[-1])    #record the stack size at the end of the round\n",
    "                \n",
    "            # Update active players in the game by removing players with stacks less than the minimum bet\n",
    "            active_players = [agent for agent in players if agent.stack > env_config['min_bet']]\n",
    "\n",
    "        tracked_agent_stack_sizes.append(game_stack_sizes)\n",
    "        \n",
    "        if len(states) == len(rewards):\n",
    "            states.append(np.zeros(5))\n",
    "        \n",
    "        states = np.array(states).astype(np.float32)    #convert states to correct datatype for torch operations\n",
    "        discounted_rewards = rewards * (agent_config['gamma'] ** np.array(range(len(rewards))))    #discount the reward history\n",
    "        causal_return = np.cumsum((discounted_rewards)[::-1])[::-1]    #compute the causal return\n",
    "        causal_return = torch.tensor(list(causal_return))    #turn into a torch tensor\n",
    "        if agent_config['baseline']:    #if we'd like the agent to use baselining...\n",
    "            baseline += sum(discounted_rewards)    #update the baseline with info from this episode\n",
    "\n",
    "        batch.append({\n",
    "            'states': states,\n",
    "            'actions': actions,\n",
    "            'rewards': rewards,\n",
    "            'total_return': sum(discounted_rewards),\n",
    "            'causal_return': causal_return\n",
    "        })    #add data from this episode to the batch\n",
    "        \n",
    "    for j in range(agent_config['N']):    #once the batch is made loop over episodes\n",
    "        batch[j]['baseline'] = baseline / agent_config['N']    #add the baseline to each one\n",
    "    tracked_agent.update_pi(batch)    #run the gradient update\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[200, 418, 220, 220, 24, 0],\n",
       " [200, 198, 25, 37, 5, 5, 1],\n",
       " [200, 0],\n",
       " [200, 198, 0],\n",
       " [200, 7, 0],\n",
       " [200,\n",
       "  530,\n",
       "  427,\n",
       "  226,\n",
       "  118,\n",
       "  236,\n",
       "  144,\n",
       "  140,\n",
       "  63,\n",
       "  112,\n",
       "  116,\n",
       "  118,\n",
       "  212,\n",
       "  187,\n",
       "  191,\n",
       "  94,\n",
       "  92,\n",
       "  94,\n",
       "  75,\n",
       "  71,\n",
       "  37,\n",
       "  39,\n",
       "  37,\n",
       "  33,\n",
       "  25,\n",
       "  21,\n",
       "  19,\n",
       "  15,\n",
       "  2],\n",
       " [200, 180, 178, 176, 35, 33, 35, 33, 0],\n",
       " [200, 49, 49, 55, 23, 51, 49, 33, 35, 33, 35, 39, 35, 39, 0],\n",
       " [200, 0],\n",
       " [200, 740, 680, 684, 653, 506, 800],\n",
       " [200, 497, 420, 40, 42, 38, 40, 38, 42, 7, 7, 11, 7, 11, 7, 5, 0],\n",
       " [200, 200, 200, 196, 381, 389, 387, 391, 453, 455, 453, 135, 97, 194, 16, 0],\n",
       " [200, 95, 192, 322, 337, 337, 0],\n",
       " [200, 0],\n",
       " [200, 198, 194, 190, 188, 212, 410, 412, 416, 418, 41, 82, 86, 172, 176, 0],\n",
       " [200, 383, 602, 800],\n",
       " [200, 9, 1],\n",
       " [200, 19, 43, 41, 88, 84, 85, 20, 8, 4],\n",
       " [200, 196, 200, 604, 796],\n",
       " [200, 431, 427, 763, 767, 798],\n",
       " [200, 20, 0],\n",
       " [200, 2],\n",
       " [200, 198, 31, 14, 10, 1],\n",
       " [200, 604, 608, 610, 614, 616, 432, 434, 68, 0],\n",
       " [200, 0],\n",
       " [200, 270, 3],\n",
       " [200, 391, 551, 547, 597, 660, 660, 686, 690, 692, 799],\n",
       " [200, 200, 196, 192, 61, 59, 30, 0],\n",
       " [200,\n",
       "  196,\n",
       "  196,\n",
       "  192,\n",
       "  276,\n",
       "  103,\n",
       "  105,\n",
       "  103,\n",
       "  23,\n",
       "  46,\n",
       "  13,\n",
       "  11,\n",
       "  7,\n",
       "  11,\n",
       "  15,\n",
       "  30,\n",
       "  32,\n",
       "  64,\n",
       "  128,\n",
       "  132,\n",
       "  134,\n",
       "  138,\n",
       "  271,\n",
       "  275,\n",
       "  277,\n",
       "  275,\n",
       "  22,\n",
       "  26,\n",
       "  0],\n",
       " [200, 200, 200, 301, 0],\n",
       " [200, 2],\n",
       " [200, 537, 543, 743, 747, 694, 690, 800],\n",
       " [200, 0],\n",
       " [200, 406, 410, 410, 218, 327, 139, 307, 329, 658, 519, 241, 0],\n",
       " [200, 2],\n",
       " [200, 17, 2],\n",
       " [200, 96, 0],\n",
       " [200, 200, 108, 1],\n",
       " [200, 200, 67, 0],\n",
       " [200, 1]]"
      ]
     },
     "execution_count": 363,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tracked_agent_stack_sizes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Old Code for running with Random Agents, useful for baselining stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:29<00:00, 33.87it/s]\n"
     ]
    }
   ],
   "source": [
    "env_config = {\n",
    "    'player_count': 4,\n",
    "    'blinds': (2, 4),\n",
    "    'min_bet': 4,\n",
    "    'starting_stack': 100\n",
    "}\n",
    "# Define the players in the game with Agent objects\n",
    "players: List[Agent] = [ExampleRandomAgent(f'Player {_}', env_config['starting_stack']) for _ in range(env_config['player_count'])]\n",
    "# Randomly select a player to be the dealer\n",
    "player_offset = np.random.randint(0, len(players))\n",
    "# Create the environment with the configuration\n",
    "env = TexasHoldemEnvironment(env_config)\n",
    "# Number of games (episodes) to play\n",
    "num_games = 1000\n",
    "\n",
    "for episode in tqdm(range(num_games)):\n",
    "    # Reset all players and environment\n",
    "    active_players = [agent.reset() for agent in players]\n",
    "    env.reset_game()\n",
    "    env.reset_round(active_players)\n",
    "    num_rounds = 0   # Number of rounds played in the game\n",
    "\n",
    "    # Play rounds until there is only one player left or 100 rounds have been played\n",
    "    while len(active_players) > 1:\n",
    "        done = False\n",
    "        # Calculate amount to offset players to get a new dealer\n",
    "        player_offset = (player_offset + 1) % len(active_players)\n",
    "        # Rotate players based on offset\n",
    "        active_players = active_players[player_offset:] + active_players[:player_offset]\n",
    "        # Reset the environment for a round of poker\n",
    "        env.reset_round(active_players)\n",
    "\n",
    "        # Play the round until the round is over\n",
    "        while not done:\n",
    "            # Get the current acting player and their action\n",
    "            current_player = active_players[env.poker_round.actor_index]\n",
    "            action = current_player.get_action(env.poker_round)\n",
    "            # Step the environment with the player's action\n",
    "            current_state = env.step(action)\n",
    "            # Update the player's observations, actions, and rewards\n",
    "            done = current_state['done']\n",
    "\n",
    "        # Update stacks for each player\n",
    "        for agent, stack in zip(active_players, env.stacks):\n",
    "            agent.stack = stack\n",
    "\n",
    "        # Update active players in the game by removing players with 0 stack\n",
    "        active_players = [agent for agent in players if agent.stack > env_config['min_bet']]\n",
    "        num_rounds += 1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "apollo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
