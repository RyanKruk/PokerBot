{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install treys\n",
    "%pip install pokerenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pokerkit import Automation, NoLimitTexasHoldem, State, calculate_hand_strength, parse_range, Card, Deck, StandardHighHand\n",
    "from typing import List\n",
    "from tqdm import tqdm\n",
    "from enum import Enum\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Agent Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Action(Enum):\n",
    "    FOLD = 0\n",
    "    CALL = 1\n",
    "    BET = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init__(self, name: str = None, starting_stack: int = 0):\n",
    "        self.name = name\n",
    "        self.actions = []\n",
    "        self.observations = []\n",
    "        self.rewards = []\n",
    "        self.starting_stack = starting_stack\n",
    "        self.stack = starting_stack\n",
    "\n",
    "    def reset(self):\n",
    "        self.actions = []\n",
    "        self.observations = []\n",
    "        self.rewards = []\n",
    "        self.stack = self.starting_stack\n",
    "        return self\n",
    "    \n",
    "    def get_action(self, state: State):\n",
    "        pass\n",
    "\n",
    "    def get_player_state(self, state: State):\n",
    "        # Calculate percent chance of winning\n",
    "        win_percent = self._calculate_strength(state)\n",
    "        \n",
    "        stack = state.stacks[state.actor_index]\n",
    "        pot = state.total_pot_amount\n",
    "\n",
    "        min_bet = state.min_completion_betting_or_raising_to_amount\n",
    "        max_bet = state.max_completion_betting_or_raising_to_amount\n",
    "        \n",
    "        return win_percent, stack, pot, min_bet, max_bet\n",
    "\n",
    "    def _get_valid_actions(self, state: State):\n",
    "        if state.can_fold():\n",
    "            yield Action.FOLD\n",
    "        if state.can_check_or_call():\n",
    "            yield Action.CALL\n",
    "        if state.can_complete_bet_or_raise_to():\n",
    "            yield Action.BET\n",
    "\n",
    "    def _calculate_strength(self, state: State, samples: int = 500):\n",
    "        return calculate_hand_strength(\n",
    "            state.player_count,\n",
    "            parse_range(''.join([str(c.rank + c.suit) for c in state.hole_cards[state.actor_index]])),\n",
    "            Card.parse(''.join([str(c[0].rank + c[0].suit) for c in state.board_cards])),\n",
    "            2,\n",
    "            5,\n",
    "            Deck.STANDARD,\n",
    "            (StandardHighHand,),\n",
    "            sample_count=samples\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExampleRandomAgent(Agent):\n",
    "    def __init__(self, name: str = None, stack: int = 1000):\n",
    "        super().__init__(name, stack)\n",
    "\n",
    "    def get_action(self, state: State) -> dict:\n",
    "        cards, board, stack, pot = self.get_player_state(state)\n",
    "\n",
    "        valid_actions = list(self._get_valid_actions(state))\n",
    "            \n",
    "        valid_bet_low = state.min_completion_betting_or_raising_to_amount\n",
    "        valid_bet_high = state.max_completion_betting_or_raising_to_amount\n",
    "        chosen_action = np.random.choice(valid_actions)\n",
    "\n",
    "        bet_size = 0\n",
    "        if chosen_action is Action.BET:\n",
    "            bet_size = round(np.random.uniform(valid_bet_low, valid_bet_high))\n",
    "\n",
    "        table_action = {\n",
    "            'table_action': chosen_action,\n",
    "            'bet_amount': bet_size\n",
    "        }\n",
    "        self.actions.append(table_action)\n",
    "        return table_action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n",
    "class PolicyNetwork(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(PolicyNetwork, self).__init__()\n",
    "        self.hidden_layer = nn.Linear(input_size, hidden_size)    #the hidden layer with hidden_size neurons\n",
    "        #nn.init.xavier_uniform_(self.hidden_layer.weight)     # Initialize the weights with Xavier initialization\n",
    "        nn.init.normal_(self.hidden_layer.weight, mean = 0, std = 0.01)\n",
    "        nn.init.normal_(self.hidden_layer.bias, mean = 0, std = 0.01)\n",
    "        self.action_output = nn.Linear(hidden_size, 3)    #the output layer with outputs as prob of stopping, mean, and variance of normal\n",
    "        self.bet_output = nn.Linear(hidden_size, 1)    #the output layer with outputs as prob of stopping, mean, and variance of normal\n",
    "\n",
    "        \n",
    "    #@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n",
    "    def forward(self, s):\n",
    "        '''A function to do the forward pass\n",
    "            Takes:\n",
    "                s -- the state representation\n",
    "            Returns:\n",
    "                a tensor of probabilities\n",
    "        '''\n",
    "        s = torch.relu(self.hidden_layer(s))    #pass through the hidden layer\n",
    "        #mu_s2 = self.output_layer(s)    #use softmax to get action probabilities\n",
    "        a = self.action_output(s)\n",
    "        a = torch.softmax(s, dim=0)\n",
    "        \n",
    "        b = self.bet_output(s)\n",
    "        b = torch.exp(b)/(1 + torch.exp(b))\n",
    "        #s = torch.softmax(s, dim=0)    #use softmax to get action probabilities\n",
    "        return a, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n",
    "class Agent():\n",
    "\n",
    "    #@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@    \n",
    "    def __init__(self, config):\n",
    "        self.config = config\n",
    "        self.mu_s2 = PolicyNetwork(5, self.config['hidden_layer_size'])    #init the policy model\n",
    "        self.optimizer = optim.Adam(self.mu_s2.parameters(), lr=self.config['learning_rate'])    #init the optimizer\n",
    "\n",
    "    #@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@   \n",
    "    def action_probs(self, a, s):\n",
    "        '''A function to compute the logged action probabilities.  This will used used for gradient updates.\n",
    "            Takes:\n",
    "                a -- -1 (stop) or float in [0,1]\n",
    "                state -- float in [0,100]\n",
    "            Returns:\n",
    "                torch tensor\n",
    "        '''\n",
    "        actions, bet_ratio = self.mu_s2(torch.tensor([s]))    #compute stop prob, mean, sd\n",
    "        if a == Action.FOLD:   #if the action was to stop...\n",
    "            p = actions[0].view(1)\n",
    "            log_p = torch.log(p)\n",
    "        elif a == Action.CALL:    #if the action was to ask for a...\n",
    "            p = actions[1].view(1)\n",
    "            log_p = torch.log(p)\n",
    "        else:    #if the action was to ask for a in [0,1]...\n",
    "            p = actions[2].view(1)\n",
    "            action_log_p = torch.log(p)\n",
    "\n",
    "            low = torch.max(bet_ratio - self.config['action_var'],torch.tensor([0.0]))\n",
    "            high = torch.min(bet_ratio + self.config['action_var'],torch.tensor([1.0]))\n",
    "            U = torch.distributions.Uniform(low, high)\n",
    "            bet_log_p = U.log_prob(torch.tensor(a['bet_ratio'])) \n",
    "\n",
    "            log_p = action_log_p + bet_log_p\n",
    "\n",
    "        return log_p\n",
    "\n",
    "    #@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@   \n",
    "    def pi_action_generator(self, s):\n",
    "        '''A function to generate an action.  This will be used to generate the data.\n",
    "            Takes:\n",
    "                state -- float in [0,100]\n",
    "            Returns:\n",
    "                 -1 or a in [0,1]\n",
    "\n",
    "        '''\n",
    "        actions, bet_ratio = self.mu_s2(torch.tensor([s]))    #generate policy parameters\n",
    "        fold, call, bet = actions[0], actions[1], actions[2]    #unpack \n",
    "        bet_size = 0    #init the bet size\n",
    "\n",
    "        action_chance = np.random.uniform()    #generate a random number to decide what to do\n",
    "        if action_chance < float(fold):    #if we choose to stop...\n",
    "            a = Action.FOLD    #set the relevant action\n",
    "        elif action_chance < float(fold) + float(call):    #if we choose to ask for a...\n",
    "            a = Action.CALL\n",
    "        else:    #if not...\n",
    "            a = Action.BET    #set the relevant action\n",
    "            min_bet_size, max_bet_size = s[-2], s[-1]    #pull out the min and max bet sizes\n",
    "\n",
    "            low = torch.max(bet_ratio - self.config['action_var'],torch.tensor([0.0]))\n",
    "            high = torch.min(bet_ratio + self.config['action_var'],torch.tensor([1.0]))\n",
    "            U = torch.distributions.Uniform(low, high)\n",
    "            bet_ratio = float(U.sample())\n",
    "\n",
    "            bet_size = (bet_ratio * (max_bet_size - min_bet_size)) + min_bet_size    #compute the bet size\n",
    "            \n",
    "        return {\n",
    "            'table_action': a,\n",
    "            'bet_size': bet_size,\n",
    "            'bet_ratio': bet_ratio\n",
    "        }\n",
    "\n",
    "    #@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n",
    "    def objective(self, log_probs, episode_return, b):\n",
    "        '''A function to compute the objective\n",
    "            Takes:\n",
    "                log_probs -- tensor, the output from the forward pass\n",
    "                causal_return -- tensor, the causal return as defined in lecture\n",
    "                b -- float, the baseline as defined in lecture\n",
    "        '''\n",
    "        return -torch.sum(log_probs * (episode_return - b))\n",
    "\n",
    "    #@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n",
    "    def update_pi(self, batch):\n",
    "        '''A function to update the gradient of the agent.\n",
    "            Takes:\n",
    "                batch -- a list of dictionary containing episode histories\n",
    "        '''\n",
    "        objective = []    #init the objectives per episode   \n",
    "        for j in range(self.config['N']):    #loop over episodes\n",
    "            batch_j = batch[j]    #pull out episode j\n",
    "            log_probs = []    #init the log probs for this episode\n",
    "            for s,a in zip(batch_j['states'][:len(batch_j['states'])-1],batch_j['actions']):    #loop over state action pairs\n",
    "                log_prob = self.action_probs(a,s)    #compute the log prob for this state action pair\n",
    "                log_probs.append(log_prob)    #record\n",
    "            log_probs = torch.stack(log_probs)    #reshape to compute gradient over the whole episode\n",
    "            if self.config['causal_return']:    #if we use causal returns...\n",
    "                batch_j_reward = batch_j['causal_return']    #set that\n",
    "            else:    #if not...\n",
    "                batch_j_reward = batch_j['total_return']    #use the total discounted reward\n",
    "            objective.append(self.objective(log_probs, batch_j_reward, batch_j['baseline']))    #compute the objective function and record\n",
    "        \n",
    "        objective = torch.mean(torch.stack(objective))    #reshape\n",
    "        \n",
    "        #run the backward pass to compute gradients\n",
    "        self.optimizer.zero_grad()    #zero gradients from the previous step\n",
    "        objective.backward()    #compute gradients\n",
    "        self.optimizer.step()    #update policy network parameters\\n\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TexasHoldemEnvironment():\n",
    "    def __init__(self, config):\n",
    "        self.config = config\n",
    "        self.reset_game()\n",
    "    \n",
    "    def reset_game(self):\n",
    "        self.game = NoLimitTexasHoldem(\n",
    "            automations= (\n",
    "                Automation.ANTE_POSTING,\n",
    "                Automation.BET_COLLECTION,\n",
    "                Automation.BLIND_OR_STRADDLE_POSTING,\n",
    "                Automation.CARD_BURNING,\n",
    "                Automation.HOLE_DEALING,\n",
    "                Automation.BOARD_DEALING,\n",
    "                Automation.HOLE_CARDS_SHOWING_OR_MUCKING,\n",
    "                Automation.HAND_KILLING,\n",
    "                Automation.CHIPS_PUSHING,\n",
    "                Automation.CHIPS_PULLING\n",
    "            ),\n",
    "            ante_trimming_status=True,  # Uniform antes?\n",
    "            raw_antes=0,  # Antes\n",
    "            raw_blinds_or_straddles=self.config['blinds'],  # Blinds\n",
    "            min_bet=self.config['min_bet'],  # Minimum bet\n",
    "        )\n",
    "        self.stacks = np.array([self.config['starting_stack'] for _ in range(self.config['player_count'])])  # Observed Agent stack will always be at position 0\n",
    "        self.poker_round = None\n",
    "        \n",
    "    def reset_round(self, agents: List[Agent]):\n",
    "        agent_stacks = [agent.stack for agent in agents]\n",
    "        self.poker_round = self.game(raw_starting_stacks=agent_stacks, player_count=agent_stacks.__len__())\n",
    "\n",
    "    def step(self, action):\n",
    "        player_action = action['table_action']\n",
    "        bet_amount = action['bet_amount']\n",
    "\n",
    "        # Update the environment state with the player action\n",
    "        if player_action == Action.FOLD:\n",
    "            self.poker_round.fold()\n",
    "        elif player_action == Action.CALL:\n",
    "            self.poker_round.check_or_call()\n",
    "        elif player_action == Action.BET:\n",
    "            self.poker_round.complete_bet_or_raise_to(bet_amount)\n",
    "        \n",
    "        done = not self.poker_round.status  # Check if the round is done\n",
    "\n",
    "        # Update the environment stacks if the round is done\n",
    "        if done:\n",
    "            self.stacks = self.poker_round.stacks\n",
    "\n",
    "        return {'state': self.poker_round, 'reward': 0, 'done': done}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Running the Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#run training loop\n",
    "performance = {'stopping_state':[],'terminal_reward':[]}\n",
    "for b in tqdm(range(agent.config['B'])):    #loop over batches\n",
    "    baseline = 0    #init the baseline\n",
    "    batch = []    #init the batch\n",
    "    for _ in range(agent.config['N']):    #loop over episodes\n",
    "        done = False    #set stopping condition\n",
    "        gamma_array = [1]    #init the discounting\n",
    "        states = [proc.state]    #init the state history\n",
    "        actions = []    #init the action history\n",
    "        rewards = []    #init the reward history\n",
    "        while not done:    #while the stopping condition is not satisfied...\n",
    "            a = agent.pi_action_generator(proc.state)    #sample an action according to the policy\n",
    "            update = proc.step(a)    #evolve the environment\n",
    "            done = update['done']    #update stopping condition\n",
    "            \n",
    "            #update the data from this episode\n",
    "            states.append(update['state'])    #record the new state\n",
    "            actions.append(a)    #record the new action\n",
    "            rewards.append(update['reward'])    #record the rewards\n",
    "            gamma_array.append(gamma_array[-1]*agent_config['gamma'])    #append another discount\n",
    "\n",
    "        states = list(np.array(states).astype(np.float32))    #convert states to correct datatype for torch operations\n",
    "        discounted_rewards = rewards*(agent_config['gamma']**np.array(range(len(rewards))))    #discount the reward history\n",
    "        causal_return = np.cumsum((discounted_rewards)[::-1])[::-1]    #compute the causal return\n",
    "        causal_return = torch.tensor(list(causal_return))    #turn into a torch tensor\n",
    "        if agent.config['baseline']:    #if we'd like the agent to use baselining...\n",
    "            baseline += sum(discounted_rewards)    #update the baseline with info from this episode\n",
    "        batch.append({'states':states\n",
    "                      ,'actions':actions\n",
    "                      ,'rewards':rewards\n",
    "                      ,'total_return':sum(discounted_rewards)\n",
    "                      ,'causal_return':causal_return})    #add data from this episode to the batch\n",
    "        performance['stopping_state'].append(proc.state)\n",
    "        performance['terminal_reward'].append(update['reward'])\n",
    "        proc.reset()    #reset the environment for the next go\n",
    "        \n",
    "    for j in range(agent.config['N']):    #once the batch is made loop over episodes\n",
    "        batch[j]['baseline'] = baseline/agent.config['N']    #add the baseline to each one\n",
    "    agent.update_pi(batch)    #run the gradient update\\n\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:29<00:00, 33.87it/s]\n"
     ]
    }
   ],
   "source": [
    "env_config = {\n",
    "    'player_count': 4,\n",
    "    'blinds': (2, 4),\n",
    "    'min_bet': 4,\n",
    "    'starting_stack': 100\n",
    "}\n",
    "# Define the players in the game with Agent objects\n",
    "players: List[Agent] = [ExampleRandomAgent(f'Player {_}', env_config['starting_stack']) for _ in range(env_config['player_count'])]\n",
    "# Randomly select a player to be the dealer\n",
    "player_offset = np.random.randint(0, len(players))\n",
    "# Create the environment with the configuration\n",
    "env = TexasHoldemEnvironment(env_config)\n",
    "# Number of games (episodes) to play\n",
    "num_games = 1000\n",
    "\n",
    "for episode in tqdm(range(num_games)):\n",
    "    # Reset all players and environment\n",
    "    active_players = [agent.reset() for agent in players]\n",
    "    env.reset_game()\n",
    "    env.reset_round(active_players)\n",
    "    num_rounds = 0   # Number of rounds played in the game\n",
    "\n",
    "    # Play rounds until there is only one player left or 100 rounds have been played\n",
    "    while len(active_players) > 1:\n",
    "        done = False\n",
    "        # Calculate amount to offset players to get a new dealer\n",
    "        player_offset = (player_offset + 1) % len(active_players)\n",
    "        # Rotate players based on offset\n",
    "        active_players = active_players[player_offset:] + active_players[:player_offset]\n",
    "        # Reset the environment for a round of poker\n",
    "        env.reset_round(active_players)\n",
    "\n",
    "        # Play the round until the round is over\n",
    "        while not done:\n",
    "            # Get the current acting player and their action\n",
    "            current_player = active_players[env.poker_round.actor_index]\n",
    "            action = current_player.get_action(env.poker_round)\n",
    "            # Step the environment with the player's action\n",
    "            current_state = env.step(action)\n",
    "            # Update the player's observations, actions, and rewards\n",
    "            done = current_state['done']\n",
    "\n",
    "        # Update stacks for each player\n",
    "        for agent, stack in zip(active_players, env.stacks):\n",
    "            agent.stack = stack\n",
    "\n",
    "        # Update active players in the game by removing players with 0 stack\n",
    "        active_players = [agent for agent in players if agent.stack > env_config['min_bet']]\n",
    "        num_rounds += 1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
